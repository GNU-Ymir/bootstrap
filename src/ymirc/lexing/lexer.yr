mod ymirc::lexing::lexer;
    
import ymirc::lexing::word;
import ymirc::lexing::tokenizer;

import std::io;
import std::collection::vec;
import std::collection::map;
import std::collection::set; 
import std::time::_;

/**
 * The lexer is the class responsible of cutting a source code string into a list of tokens
 * Unlike simple tokenizer, the lexer has context information, and switch (allowing to ignore parts of the source code, or not depending of the context)
 */
pub class @final Lexer {
    
    /// The name of the file that the lexer is reading
    let _filename : [c8];

    /// The content of the file, that is tokenized
    let _content : [c8];

    /// The list of tokens to skip
    let mut _skips : [[c8]] = [];//}::new ()

    /// Does the lexer has to skip comments
    let mut _doComments = true;

    /// The position of the cursor
    let mut _cursor = 0us;

    /// The current word that is being read
    let mut _wordCursor = 0us;

    /// The list of words infos
    let mut _wordInfos : [(usize, bool, [c8])] = [];

    /// The end comments that are found
    let dmut _commentEnds = HashMap!{usize, (usize, usize)}::new ();

    /// The list of allocated words found in the files (the tokens basically)
    let mut _words : [&Word] = [];
    
    /// The position of the (cursors, line, col, lineSeek) of the previous read words 
    let dmut _rewinders = Vec!{(usize, usize)}::new ();

    /// Set to true, when nothing is left to read (iif _cursor >= _content.len)
    let mut _eof : bool = false;

    /// The word returns when the lexer is eof
    let _eofWord : &Word;

    /**
     * Create a new lexer, ready to split content
     * @params: 
     *    - content: the content to split
     *    - tokens: the list of tokens that split the string, (cf. Tokenizer)
     *    - comments: the list of tokens that start a comment line (by default [])
     *    - skips: the list of tokens that will be omitted by the lexer when reading (by default [" ", "\n", "\t", "\r"])
     * @warning: if the skips and comments token are not in tokens, they are added, so they split the content 
     */
    pub self (filename : [c8], content : [c8], tokens: [[c8]] = [" "s8], comments : [([c8], [c8])] = [], skips: [[c8]] = [" "s8, "\n"s8, "\t"s8, "\r"s8])
        with _filename = filename,
        _content = content, _eofWord = Word::eof (filename-> filename)
    {
        self._rewinders:.push ((self._cursor, self._wordCursor));
        
        let dmut tzer = Tokenizer::new (tokens-> tokens);
        for i in comments {
            tzer:.insert (i._0, isComment-> i._1);
        }

        for i in skips {
            tzer:.insert (i, isSkip-> true);
        }
        
        self._skips = skips;
        self:.tokenize (content, alias tzer);
    }        

    /**
     * Read the next word in the lexer, and returns it, with the documentation read before it
     * @returns: 
     *   - .0: the next word in the lexer
     *   - .1: the associated comments
     */
    pub def next (mut self)-> (&Word, [c8]) {
        if (self._eof) return (self._eofWord, ""s8);
        
        self._rewinders:.push ((self._cursor, self._wordCursor));        
        let mut start_com = 0us;
        let mut end_com = 0us;
        return loop {            
            if (self._wordCursor < self._wordInfos.len) {
                let (wd, wasSkip, isComment) = self._wordInfos [self._wordCursor];
                
                self._wordCursor += 1us;
                self._cursor += wd;
                let isSkip =  wasSkip && (self._skips.len != 0us);                
                
                if (self._doComments) {
                    if (isComment != ""s8) {
                        start_com = self._cursor - wd;
                        self:.gotoNextToken (isComment);
                        end_com = self._cursor;
                    } else {                   
                        if (!isSkip) {
                            return (self._words[self._wordCursor - 1us], self._content [start_com .. end_com]);                    
                        }                        
                    }
                } else {
                    if (!isSkip) {
                        return (self._words[self._wordCursor - 1us], ""s8);                    
                    }
                }
            } else {
                self._eof = true;
                break (self._eofWord, ""s8);
            }
        };
    } 
    
    /***
     * ============================================================================
     * ============================================================================
     * =========================          SETTERS        ==========================
     * ============================================================================
     * ============================================================================
     */

    /**
     * Tell to the lexer if it must skip the 'skips' or not
     * @info: 
     *   - by default the lexer skips them
     *   - This function is used to disable token skipping when reading string content for example
     */
    pub def doComments (mut self, d : bool) -> void {
        self._doComments = d;
    }

    /**
     * Set the list of skip tokens
     * @params:
     *  - skips: the list of token to skip
     */
    pub def setSkips (mut self, skips : [[c8]]) {            
        // self._skips:.clear ();
        
        // for i in skips {
        //     self._skips:.insert (i);
        // }
        self._skips = skips;
    }
    
    /**
     * Rewind to a previous location in the file
     * Each time the function next is called, the lexer saves the cursor position
     * This function rewind go to that cursor position, so if we rewind 10 times, we will get the last 10 next calls
     * @warning: 
     * ===============
     * when toggeling skips, and doComment, the rewind does not garantee, that the lexer will return at least nb tokens
     * ===============
     */
    pub def rewind (mut self, nb : u64 = 1u64) {
        if (self._eof) {
            self._rewinders:.pop (1u64);
            self._eof = false;
        }
        
        if (nb > 1u64) self._rewinders:.pop (nb - 1u64);    
        
        let (x, h) = self._rewinders[][self._rewinders.len () - 1us] ;
        self._cursor = x;
        self._wordCursor = h;
                
        self._rewinders:.pop (1u64);    
        if (self._rewinders.len () == 0us) {
            self._rewinders:.push ((self._cursor, self._wordCursor));
        }

        
    }

    /**
     * Rewind to a previous location in the file, moving the wordCursor
     * @params: 
     *   - nb: the word cursor to go to 
     */
    pub def rewindTo (mut self, nb : u64) {
        if (self._eof) {
            self._rewinders:.pop (1u64);
            self._eof = false;
        }

        while cast!u64 (self._rewinders.len ()) > nb {
            let (x, h) = self._rewinders[][self._rewinders.len () - 1us] ;
            self._cursor = x;
            self._wordCursor = h;
            self._rewinders:.pop (1u64);            
        }

               
        if (self._rewinders.len () == 0us) {
            self._rewinders:.push ((self._cursor, self._wordCursor));
        }
    }

    /**
     * Rewind to a previous location in the file, moving the c8 cursor
     * @params: 
     *   - cursor: the c8 seek cursor to go to 
     */
    pub def rewindToSeek (mut self, cursor : u64) {
        if (self._eof) {
            self._rewinders:.pop (1u64);
            self._eof = false;
        }
                
        loop {
            if (self._rewinders.len () == 0us) break {}            
            let (x, h) = self._rewinders[][self._rewinders.len () - 1us] ;
            self._cursor = x;
            self._wordCursor = h;
            self._rewinders:.pop (1u64);
            
            if (x <= cursor) break {}
        }

        if (self._rewinders.len () == 0us) {
            self._rewinders:.push ((self._cursor, self._wordCursor));
        }
    }

    /***
     * ============================================================================
     * ============================================================================
     * =========================          GETTERS        ==========================
     * ============================================================================
     * ============================================================================
     */
    
    /**
     * @returns: the counter of previous "self:.next" calls
     * @info: this information can be usefull to go back rapidely
     * @example: 
     * =================
     * let dmut lex : &Lexer = ...
     * let read_Nb : u64 = ...
     * let counter = lex.getCounter ();
     * for i in 0 .. read_Nb {
     *     println (lex:.next ()._0);
     * }
     * lex:.rewind (nb-> lex.getCounter () - counter); // go back of number of valid read done
     * // so the lexer will be reset, as if the for loop never executed
     * =================
     */
    pub def getCounter (self)-> u64 {
        cast!u64 (self._rewinders.len ())
    }

    

    /***
     * ============================================================================
     * ============================================================================
     * =========================          PRIVATE        ==========================
     * ============================================================================
     * ============================================================================
     */
    
    /**
     * Move the cursor (and counters) to the next token == end
     */
    prv def gotoNextToken (mut self, end : [c8])-> void {
        let start = self._wordCursor;
        match self._commentEnds.find (self._wordCursor) {
            Ok (x : _) => {
                self._wordCursor = x._0;
                self._cursor = x._1;
                return {}
            }
            _ => {
                loop {
                    if (self._wordCursor < self._wordInfos.len) {
                        let (wd, _, _) = self._wordInfos [self._wordCursor];                
                        let ret = self._content [self._cursor .. self._cursor + wd];
                        self._wordCursor += 1us;
                        self._cursor += wd;
                        
                        if (ret == end) {
                            self._commentEnds:.insert (start, (self._wordCursor, self._cursor));
                            break {}
                        }
                    } else {
                        self._commentEnds:.insert (start, (self._wordCursor, self._cursor));
                        break {}
                    }
                }
            }
        }
    }

    /**
     * Tokenize the content of the file
     * @params: 
     *    - content: the content of the file to tokenize
     *    - tzer: the tokenizer already set
     */
    prv def tokenize (mut self, content : [c8], dmut tzer : &Tokenizer) {
        let mut cursor = 0us;
        let dmut words = Vec!{(usize, bool, [c8])}::new ();
        let dmut allocWords = Vec!{&Word}::new ();
        words:.reserve (content.len / 2us);
        allocWords:.reserve (content.len / 2us);
                
        let mut line = 1u64, mut col = 1u64, mut lineSeek = 0us;
        loop {
            let (len, isSkip, isComment) = tzer:.next (content [cursor .. $]);
            
            if (len != 0us) {
                let ret = content [cursor .. cursor + len];
                let (old_l, old_c) = (line, col);
                if (ret[0us] == '\n'c8) {
                    lineSeek = lineSeek + cast!usize (col);
                    line += 1u64;
                    col = 1u64;
                } else { col += len; }
                
                words:.push ((len, isSkip, isComment));
                allocWords:.push (Word::new (ret, filename-> self._filename, fileContent-> content, old_l, old_c, cursor, lineSeek));                
            } else break {}
            cursor += len;
        }

        self._wordInfos = words[];
        self._words = allocWords[];        
    }
    
    /***
     * ==================================================================================
     * ==================================================================================
     * =========================          MISCELLANEOUS        ==========================
     * ==================================================================================
     * ==================================================================================
     */
    
    impl Streamable;
    
    
}
