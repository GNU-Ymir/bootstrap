mod ymirc::lexing::lexer
    
import ymirc::lexing::word;
import ymirc::lexing::tokenizer;

import std::io;
import std::collection::vec;
import std::collection::map;
import std::collection::set; 

/**
 * The lexer is the class responsible of cutting a source code string into a list of tokens
 * Unlike simple tokenizer, the lexer has context information, and switch (allowing to ignore parts of the source code, or not depending of the context)
 */
pub class @final Lexer {

    /// The tokenizer that is able to cut a string into tokens
    let dmut _tzer : &Tokenizer;

    /// The name of the file that the lexer is reading
    let _filename : [c8];

    /// The content of the file, that is tokenized
    let _content : [c8];

    /// The list of comments { begin => end }
    let dmut _comments = HashMap!{[c8], [c8]}::new ();

    /// The list of tokens to skip
    let dmut _skips = HashSet!{[c8]}::new ()

    /// Does the lexer has to skip comments
    let mut _doComments = true;

    /// The current line of the lexer
    let mut _line = 1u64;

    // The current column
    let mut _col = 1u64;

    /// The position of the cursor
    let mut _cursor = 0us;

    /// The position of the (cursors, line, col, lineSeek) of the previous read words 
    let dmut _rewinders = Vec!{(usize, u64, u64, usize)}::new ();

    /// The position of the cursor to the beginning of the current line
    let dmut _lineSeek = 0us;

    /// Set to true, when nothing is left to read (iif _cursor >= _content.len)
    let mut _eof : bool = false;

    /**
     * Create a new lexer, ready to split content
     * @params: 
     *    - content: the content to split
     *    - tokens: the list of tokens that split the string, (cf. Tokenizer)
     *    - comments: the list of tokens that start a comment line (by default [])
     *    - skips: the list of tokens that will be omitted by the lexer when reading (by default [" ", "\n", "\t", "\r"])
     * @warning: if the skips and comments token are not in tokens, they are added, so they split the content 
     */
    pub self (filename : [c8], content : [c8], tokens: [[c8]] = [" "s8], comments : [([c8], [c8])] = [], skips: [[c8]] = [" "s8, "\n"s8, "\t"s8, "\r"s8])
        with _filename = filename,
        _tzer = Tokenizer::new (tokens-> tokens),
        _content = content
    {        
        self._rewinders:.push ((self._cursor, self._line, self._col, self._lineSeek));
        for i in comments {
            self._comments:.insert (i._0, i._1);
            self._tzer:.insert (i._0, isComment-> i._1);
        }

        for i in skips {
            self._skips:.insert (i);
            self._tzer:.insert (i, isSkip-> true);
        }                
    }        

    /**
     * Read the next word in the lexer, and returns it, with the documentation read before it
     */
    pub def next (mut self)-> (&Word, [c8]) {
        if (self._eof) return (Word::eof (filename-> self._filename), ""s8);
        
        self._rewinders:.push ((self._cursor, self._line, self._col, self._lineSeek));
        let mut start_com = 0us;
        let mut end_com = 0us;
        return loop {
            let (wd, isSkip, isComment) = self._tzer.next (self._content [self._cursor .. $]);
            if (wd != 0us) {
                let ret = self._content [self._cursor .. self._cursor + wd];
                self._cursor += wd;
                let (old_l, old_c) = (self._line, self._col);
                self:.incrementLine (ret);
                
                if (self._doComments) {                                       
                    if (isComment != ""s8) {
                        start_com = self._cursor - wd;
                        self:.gotoNextToken (isComment);
                        end_com = self._cursor;
                    } else {                   
                        if (!isSkip) {
                            return (Word::new (str-> ret, filename-> self._filename, fileContent-> self._content, old_l, old_c, self._cursor - wd, self._lineSeek), self._content [start_com .. end_com]);                    
                        }                        
                    }
                } else {
                    if (!isSkip) {
                        return (Word::new (str-> ret, filename-> self._filename, fileContent-> self._content, old_l, old_c, self._cursor - wd, self._lineSeek), ""s8);                    
                    }
                }
            } else {
                self._eof = true;
                break (Word::eof (filename-> self._filename), ""s8);
            }
        };
    } catch {
        _ => {
            self._eof = true;
            return (Word::eof (filename-> self._filename), ""s8);
        }
    }

    /***
     * ============================================================================
     * ============================================================================
     * =========================          SETTERS        ==========================
     * ============================================================================
     * ============================================================================
     */

    /**
     * Tell to the lexer if it must skip the 'skips' or not
     * @info: 
     *   - by default the lexer skips them
     *   - This function is used to disable token skipping when reading string content for example
     */
    pub def doComments (mut self, d : bool) -> void {
        self._doComments = d;
    }

    /**
     * Set the list of skip tokens
     * @params:
     *  - skips: the list of token to skip
     */
    pub def setSkips (mut self, skips : [[c8]]) {
        for i in self._skips {
            self._tzer:.insert (i, isSkip-> false);
        }
        
        self._skips:.clear ();
        
        for i in skips {
            self._skips:.insert (i);
            self._tzer:.insert (i, isSkip-> true);
        }
    }

    /**
     * Rewind to a previous location in the file
     * Each time the function next is called, the lexer saves the cursor position
     * This function rewind go to that cursor position, so if we rewind 10 times, we will get the last 10 next calls
     * @warning: 
     * ===============
     * when toggeling skips, and doComment, the rewind does not garantee, that the lexer will return at least nb tokens
     * ===============
     */
    pub def rewind (mut self, nb : u64 = 1u64) {
        if (self._eof) {
            self._rewinders:.pop ()?;
            self._eof = false;
        }
        
        for _ in 0u64 .. nb {
            {
                let (x, y, z, w) = self._rewinders [self._rewinders.len () - 1us] ;
                self._cursor = x;
                self._line = y;
                self._col = z;
                self._lineSeek = w;
                self._rewinders:.pop ();
            }?;
        };

        if (self._rewinders.len () == 0us) {
            self._rewinders:.push ((self._cursor, self._line, self._col, self._lineSeek));
        }
    }

    /***
     * ============================================================================
     * ============================================================================
     * =========================          GETTERS        ==========================
     * ============================================================================
     * ============================================================================
     */
    
    /**
     * @returns: the counter of previous "self:.next" calls
     * @info: this information can be usefull to go back rapidely
     * @example: 
     * =================
     * let dmut lex : &Lexer = ...
     * let read_Nb : u64 = ...
     * let counter = lex.getCounter ();
     * for i in 0 .. read_Nb {
     *     println (lex:.next ()._0);
     * }
     * lex:.rewind (nb-> lex.getCounter () - counter); // go back of number of valid read done
     * // so the lexer will be reset, as if the for loop never executed
     * =================
     */
    pub def getCounter (self)-> u64 {
        cast!u64 (self._rewinders.len ())
    }

    

    /***
     * ============================================================================
     * ============================================================================
     * =========================          PRIVATE        ==========================
     * ============================================================================
     * ============================================================================
     */
    

    /**
     * Increment the line, col, and lineSeek counters
     */
    prv def incrementLine (mut self, txt : [c8]) {
        for i in txt {
            if (i == '\n'c8) {
                self._lineSeek = self._lineSeek + cast!usize (self._col);
                self._line += 1u64;
                self._col = 1u64;
            } else {
                self._col += 1u64;
            }
        }
    }

    /**
     * @returns: 
     * =============
     * If the txt is the beginning of a comment, this function returns, the token marking the end of the comment
     * otherwise it returns (false, "")
     * =============
     */
    prv def isComment (self, txt : [c8]) -> (bool, [c8]) {
        let x = self._comments.find (txt);
        match x {
            Ok (z:_)=> return (true , z);
        }        
        
        (false, ""s8)
    }

    /**
     * Move the cursor (and counters) to the next token == end
     */
    prv def gotoNextToken (mut self, end : [c8])-> void {
        loop {
            let (wd, _, _) = self._tzer.next (self._content [self._cursor .. $]);
            if wd != 0u64 {
                let ret = self._content [self._cursor .. self._cursor + wd];
                self._cursor += wd;
                self:.incrementLine (ret);
                if (ret == end) break {}
            } else {
                break {}
            }
        }
    } catch {
        _ => { }
    }

    /***
     * ==================================================================================
     * ==================================================================================
     * =========================          MISCELLANEOUS        ==========================
     * ==================================================================================
     * ==================================================================================
     */
    
    impl Streamable;
    
    
}



