/*
 * Declare the SrcTokenizer record to help splitting the src content into
 * tokens, and store some utils to check string information, comment
 * information, isToken, isKeyword, etc.
 *
 * <hr>
 * 
 * @Package: ymirc        
 * @Module: ymirc::lexing::tokenizer
 * @File: ymirc/lexing/tokenizer.yr
 * @Author(s):
 *   - Emile Cadorel <ecadorel@gmail.com> 
 *
 * @Created (YYYY-MM-DD): 2026-02-09
 * @Copyright (C) 2021â€“2026 GNU-Ymir
 * @License
 *   ```text
 *   This file is part of the Ymir language project.
 *
 *   Ymir is free software: you can redistribute it and/or modify it
 *   under the terms of the GNU General Public License as published by
 *   the Free Software Foundation, either version 3 of the License, or
 *   (at your option) any later version.
 *
 *   Ymir is distributed in the hope that it will be useful,
 *   but WITHOUT ANY WARRANTY; without even the implied warranty of
 *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *   GNU General Public License for more details.
 *
 *   You should have received a copy of the GNU General Public License
 *   along with Ymir.  If not, see <https://www.gnu.org/licenses/>.
 *   ```
 */

in tokenizer;

mod ::implem;
mod ::node;
mod ::slice;

use ymirc::lexing::{keys, tokens};

/**
 * The source tokenizer is immutable element once initialized, but
 * initialization takes time and memory, so initializing it only one time is
 * enough
 * 
 * */
pub lazy __SRC_TOKENIZER__ = SrcTokenizer ();


pub def Tokenizer : implem::Tokenizer;
pub def Slice     : slice::Slice;

/**
 * A tokenizer utils that contains useful informations for the lexer
 * */
pub record SrcTokenizer {

    /*! Private fields */
    prv {        
        // The tokenizer
        let mut _tzer: Tokenizer;

        // end of comment and strings to manage nested cuts
        let mut _stringComEnds: [[c8] => mut [[c8] => mut ()]] = copy [];

        // the list of starters of strings. Tokenizer does not manage strings
        // tagging, so we will use it's comment tagging and check wether it's a
        // string or a comment.
        // content: open => (close, multiline)
        let mut _stringInfos: [[c8] => mut ([c8], bool, [[c8]], bool)] = copy [];

        // The informations about comments to decide what to do with them
        // content: open => (close, multiline, doc)
        let mut _commInfos: [[c8] => mut ([c8], bool, bool)] = copy [];

        // The set of keywords
        let mut _keywords: [[c8] => mut ()] = copy [];
        
    }

    /*!
     * ====================================================================================================
     * ====================================================================================================
     * ======================================          CTOR          ======================================
     * ====================================================================================================
     * ====================================================================================================
     */
    
    pub self ()        
        with _tzer = Tokenizer (copy Tokens::__members__)        
    {
        self:.configure ();
    }

    /*!
     * ====================================================================================================
     * ====================================================================================================
     * ===================================          SPLITTERS          ====================================
     * ====================================================================================================
     * ====================================================================================================
     */

    /**
     * Find the next token in `str`
     *
     * @returns: the splitted token
     * */
    pub fn next (self, str: [c8])-> Slice {
        let (len, isToken, isSkip, isComment, isString) = self._tzer.next (str);
        if len == 0 // Empty split, EOF
            return Slice ();

        let mut split = Slice ();
        split.tok = str [0 .. len];
        split.isToken = isToken;
        split.isSkip = isSkip;
        split.isString = isString;
        split.isComment = isComment;

        if !split.isToken {
            split.isKeyword = (split.tok in self._keywords);
        }

        split        
    }
            
    /*!
     * ====================================================================================================
     * ====================================================================================================
     * ====================================          GETTERS          =====================================
     * ====================================================================================================
     * ====================================================================================================
     */
    
    /**
     * Find the string info from the opening token
     * 
     * @returns:
     *    - .0: the closing token
     *    - .1: true iif multiline
     *    - .2: the list of suffixes for that string opener
     * */
    pub fn stringInfo (self, tok: [c8])-> ([c8], bool, [[c8]], bool) {
        if let Ok (x) = self._stringInfos [tok] {
            x
        } else {
            ("", false, [], false)
        }
    }    
    
    /**
     * Find the comment info from the opening token
     * 
     * @returns:
     *    - .0: the closing token
     *    - .1: true iif multiline
     *    - .2: true iif keep in documentation
     * */
    pub fn commentInfo (self, tok: [c8])-> ([c8], bool, bool) {
        if let Ok (x) = self._commInfos [tok] {
            x
        } else {
            ("", false, false)
        }
    }

    /**
     * @returns: the list of tokens that can open a nest comment/string closing with `closing`
     * */
    pub fn openList (self, closing: [c8])-> [[c8] => ()] {
        if let Ok (x) = self._stringComEnds [closing] {
            x
        } else {
            copy []
        }
    }
    
    /*!
     * ====================================================================================================
     * ====================================================================================================
     * =================================          CONFIGURATION          ==================================
     * ====================================================================================================
     * ====================================================================================================
     */

    /**
     * Configure the tokenizer with the keywords and tokens
     * */
    fn configure (mut self) {
        self:.configureCommentInfos ();
        self:.configureStringInfo ();
        self:.configureSkipInfos ();
        self:.configureKeywords ();        
    }


    /**
     * Configure the comment accepted in an Ymir source file, and how the
     * tokenizer should handle them
     * 
     * */
    fn configureCommentInfos (mut self) {
        self._tzer:.insert (Tokens::LCOMM1, isComment-> true);
        self._tzer:.insert (Tokens::LCOMM2, isComment-> true);
        self._tzer:.insert (Tokens::LCOMM3, isComment-> true);

        // Nested multiline comments are allowed
        self._stringComEnds [Tokens::RCOMM2] = copy [Tokens::LCOMM1 => (), Tokens::LCOMM2 => ()];

        // .0 = end, .1 = multiline, .2 = export doc?
        self._commInfos [Tokens::LCOMM1] = (Tokens::RETURN, false, true);  // //         
        self._commInfos [Tokens::LCOMM2] = (Tokens::RCOMM2, true, true); // /* */
        self._commInfos [Tokens::LCOMM3] = (Tokens::RCOMM2, true, false); // /*! */        
    }

    /**
     * Configure the string accepted in an Ymir source file and how the
     * tokenizer shoud handle them
     * 
     * */
    fn configureStringInfo (mut self) {
        self._tzer:.insert (Tokens::TRIPLE_DQUOTE, isString-> true);
        self._tzer:.insert (Tokens::BACK_QUOTE, isString-> true);
        self._tzer:.insert (Tokens::DQUOTE, isString-> true);
        self._tzer:.insert (Tokens::SQUOTE, isString-> true);
        self._tzer:.insert (Tokens::MACRO_ACC, isString-> true);
        self._tzer:.insert (Tokens::MACRO_PAR, isString-> true);
        self._tzer:.insert (Tokens::MACRO_CRO, isString-> true);

        // nested {} and #{} are allowed withing #{}
        // nested are not allowed in others string literals but macros 
        self._stringComEnds [Tokens::RACC] = copy [Tokens::LACC => (), Tokens::MACRO_ACC => ()];
        self._stringComEnds [Tokens::RPAR] = copy [Tokens::LPAR => (), Tokens::MACRO_PAR => ()];
        self._stringComEnds [Tokens::RCRO] = copy [Tokens::LCRO => (), Tokens::MACRO_CRO => ()];        


        let strSuffixes = copy StringSuffixes::__members__;
        let charSuffixes = copy CharSuffixes::__members__;
        
        // .0 = end, .1 = multiline, .2 = suffixes, .3 = isIdentifier?
        self._stringInfos [Tokens::TRIPLE_DQUOTE] = (Tokens::TRIPLE_DQUOTE, true,  strSuffixes,  false); // """str"""
        self._stringInfos [Tokens::DQUOTE]        = (Tokens::DQUOTE       , false, strSuffixes,  false); // "str"
        self._stringInfos [Tokens::SQUOTE]        = (Tokens::SQUOTE       , false, charSuffixes, false); // 'char'
        
        self._stringInfos [Tokens::BACK_QUOTE] = (Tokens::BACK_QUOTE, false, [], true); // `ident`
        
        self._stringInfos [Tokens::MACRO_ACC] = (Tokens::RACC, true, [], false); // #{macro}
        self._stringInfos [Tokens::MACRO_PAR] = (Tokens::RPAR, true, [], false); // #(macro)
        self._stringInfos [Tokens::MACRO_CRO] = (Tokens::RCRO, true, [], false); // #[macro]                
    }

    /**
     * Configure the tokens that are skipped in a source file
     * 
     * */
    fn configureSkipInfos (mut self) {        
        for i in SkipTokens::__members__ {
            self._tzer:.insert (i, isSkip-> true);
        }
    }

    /**
     * Configure the list of tokens that needs to be marked as keywords
     * 
     * */
    fn configureKeywords (mut self) {        
        for k in ForbiddenKeys::__members__ {
            self._keywords [k] = ();
        }
    }

}
