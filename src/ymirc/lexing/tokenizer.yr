mod ymirc::lexing::tokenizer;

import core::object;
import core::array;
import core::typeinfo;
import core::exception;

import std::collection::map;
import std::collection::vec;
import std::io;

/**
 * A tokenizer is an enhanced string splitter, that splits strings using tokens instead of just chars
 * Tokenizer are really usefull for grammar visitor, and can be associated with Lexers pretty easily
 * @example: 
 * ============
 * // using a tokenizer, tokens can be multiple letter long, and there can be collision between tokens
 * // For example, the token '=' and '=>' won't be a problem for the tokenizer
 * let dmut tzer = Tokenizer::new (tokens-> ["("s8, ")"s8, "=>"s8, ","s8, "="s8, "+"s8, "*"s8]);
 * // set a skip token
 * tzer:.insert (" "s8, isSkip-> true); 
 *
 * // insert a comment token
 * tzer:.insert ("#"s8, isComment-> "#"s8);
 *
 * let mut cursor = 0u64;
 * let str = "(x, y) => # this is a comment # x + y * 2"s8;
 * loop {
 *    let (len, isSkip, isComment) = tzer.next (str [cursor .. $]);
 *    if len != 0u64 {
 *        println (format ("%(y) -> %(b), %(r)"s8, str [cursor .. cursor + len], isSkip, isComment));
 *    } else break {}
 * }
 * ============
 */
pub class @final Tokenizer {

    prv let dmut _heads = HashMap!{c8, &internal::Node}::new ();

    /**
     * Create a new tokenizer, with a set of tokens
     * @params: 
     *   - tokens: the list of token that will split the string
     * @example: 
     * ============
     * let dmut tzer = Tokenizer::new (tokens-> ["(", ")", "=>", ":", "<", ">", ",", " "]);
     * let str = "(x, y) => x > y";
     * let lst = tzer.tokenize (str);
     * assert (lst == ["(", "x", ",", " ", "y", ")", " ", "=>", " ", "x", " ", ">", " ", "y"]); 
     * ============
     */
    pub self (tokens: [[c8]] = []) {
        for i in tokens {
            self:.insert (i, isSkip-> false, isComment-> ""s8);
        }
    }
    
    /**
     * Insert a new token in the tokenizer
     * @params: 
     *    - token: the token to insert
     * @example: 
     * ================
     * let dmut tzer = Tokenizer::new ();
     * tzer:.insert ("+");
     * tzer:.insert ("+=");
     * tzer:.insert (" ");
     * let lst = tzer.tokenize ("x += y");
     * assert (lst == ["x", " ", "+=", " ", "y"]);
     * ================
     */
    pub def insert (mut self, token : [c8], isSkip : bool = false, isComment : [c8] = ""s8) {
        if (token.len != 0us) {
            {               
                let fnd = (self._heads.find (token[0]));
                match fnd {
                    Ok (x:_) => {
                        (alias self._heads) [token[0]] = x.insert (token[1us .. $], isSkip-> isSkip, isComment-> isComment);                    
                    }
                    _ => {
                        (alias self._heads) [token[0]] = internal::Node::new (token[0], isSkip-> false, isComment-> ""s8).insert (token [1us .. $], isSkip-> isSkip, isComment-> isComment);
                    }
                }
            } catch {
                _ : &OutOfArray => { } // impossible
            }
        }
    }    
    
    /**
     * @returns: 
     *   - the length of the next token inside the str
     *   - true, if the token is a skip token false otherwise
     *   - if the token is a comment starter, it returns the comment ender, ""s8 otherwise
     * @example: 
     * ============
     * let dmut tzer = Tokenizer::new (["+", " "]);
     * let mut str = "fst + scd";
     * let mut len = tzer.next (str)._0;
     * assert (len == 3u64); // "fst"
     * 
     * str = str [len .. $];     
     * len = tzer.next (str)._0;
     * assert (len == 1u64); // " "
     *
     * str = str [len .. $];     
     * len = tzer.next (str)._0;
     * assert (len == 1u64); // "+"
     *
     * str = str [len .. $];     
     * len = tzer.next (str)._0;
     * assert (len == 1u64); // " "
     *
     * str = str [len .. $];     
     * len = tzer.next (str)._0;
     * assert (len == 3u64); // "scd" 
     *
     * str = str [len .. $];     
     * len = tzer.next (str)._0;
     * assert (len._0 == 0u64); 
     * ============
     */
    pub def next (self, str : [c8])-> (usize, bool, [c8]) {
        for i in 0us .. str.len {
            {
                let fnd = (self._heads.find (str [i]));
                match fnd { 
                    Ok (x:_) => { // a possible token at index == i
                        let (len, isSkip, isComment) = x.len (str [1us .. $]); // get the length of the token
                        // if the len is 0, then it is not really a token, it just start like one
                        if (len != 0us) {
                            if (i == 0us) { 
                                return (len, isSkip, isComment); // it is totally a token, we return its length
                            } else {
                                // it is a token, but there is something before it, so we return the len of the thing before it
                                return (i, false, ""s8); 
                            }
                        }
                        // it was not a token, just started like one, we continue
                    }
                }
            } catch {
                _ : &OutOfArray=>  {} // impossible
            }
        }
        
        // No token in the str, return the len of the str
        return (str.len, false, ""s8);
    }    

    impl Streamable;
    
}

mod internal {

    /**
     * A node of a tokenizer, that stores information about tokens
     */
    pub class @final Node {

        // The current value of the node
        let _key : c8; 

        // Can terminate a token? or is part of bigger tokens
        let _isToken : bool = false;

        // The list of possible continuation of the token
        let _heads : &HashMap!{c8, &Node};

        let _isSkip : bool;

        let _isComment : [c8];
        
        /**
         * Construct a new Token node
         * @params: 
         *   - key: the value of the node
         *   - isToken: can terminate a token 
         *   - heads: the list of possible continuation
         */
        pub self (key : c8, isToken : bool = false, heads : &HashMap!{c8, &Node} = {HashMap!{c8, &Node}::new ()}, isSkip : bool, isComment : [c8]) with _key = key, _isToken = isToken, _heads = heads, _isSkip = isSkip, _isComment = isComment
        {}
        
        /**
         * Insert sub tokens accepted tokens
         * @params: 
         *     - str: the rest to read to create a valid token
         * @example: 
         * ==============
         * // let say that "[+]" is a token, but "[" is not, nor is "[+"
         * let mut node = Node::new ('['); 
         * node = node.insert ("+]"); 
         * println (node); // [:false, +:false, ]:true 
         * // In that configuration the only token that will be accepted is "[+]"
         * assert (node.len ("[+]") == 3); // accepted
         * assert (node.len ("[") == 0u64); // not accepted
         * assert (node.len ("[+") == 0u64); // not accepted
         * 
         * // Now we want to accept "[-]"
         * node = node.insert ("-]");
         * // and simply "["
         * node = node.insert (""); 
         * println (node); // [:true, 
         *                 //     +:false, ]:true 
         *                 //     -: false, ]:true
         * 
         * assert (node.len ("[+]") == 3); // still accepted
         * assert (node.len ("[") == 1u64); // accepted this time
         * assert (node.len ("[+") == 1u64); // accept only the '['
         * assert (node.len ("[-]") == 3); // accepted
         * assert (node.len ("[-") == 1u64); // accept only the '['
         * ==============
         */
        pub def insert (self, str : [c8], isSkip : bool, isComment : [c8]) -> &Node {
            if (str.len == 0u64) {
                return Node::new (self._key, isToken-> true, heads-> self._heads, isSkip-> isSkip, isComment-> isComment)
            }

            let dmut retDict = HashMap!{c8, &Node}::new ();
            for i, j in self._heads {
                retDict:.insert (i, j);                    
            }

            {
                let fnd = retDict.find (str [0])
                    match fnd {
                        Ok (x:_) => {
                            retDict:.insert (str [0], x.insert (str [1us .. $], isSkip, isComment));
                        }
                        _ => {                    
                            retDict:.insert (str [0], Node::new (str [0], false, ""s8).insert (str [1us .. $], isSkip, isComment));
                        }
                    }
            } catch {
                _: &OutOfArray => {}
            }
            
            return Node::new (self._key, isToken-> self._isToken, heads-> retDict, isSkip-> isSkip, isComment-> isComment);
        }

        

        /**
         * @returns: the length of the token at the beginning of the string content
         * @example: 
         * =================
         * let mut node = Node::new ('+', isToken-> true);
         * node = node.insert ("=");
         * // Our grammar accept the tokens, "+" and "+="
         * assert (node.len ("+")._0 == 1u64); // "+" are accepted
         * assert (node.len ("+=")._0 == 2u64); // "+=" are accepted
         * assert (node.len (" +=")._0 == 0u64); // " +=" are not accepted
         * assert (node.len ("+ and some rest")._0 == 1u64); // " +" are accepted
         * =================
         */
        pub def len (self, content : [c8])-> (usize, bool, [c8]) {
            if (content.len == 0us) {
                if (self._isToken)
                    return (1us, self._isSkip, self._isComment);
                else return (0us, false, ""s8);
            }

            {
                let fnd = (self._heads.find (content [0]));
                match fnd {
                    Ok (x:_) => {
                        let (sub_len, isSkip, isComment) = x.len (content [1us .. $]);
                        if (sub_len != 0us)
                            return (1us + sub_len, isSkip, isComment);
                    }
                }
            }  catch {
                _: &OutOfArray => {} // impossible
            }

            if (self._isToken)
                return (1us, self._isSkip, self._isComment);
            return (0us, false, ""s8);
        }

        /**
         * @returns: the key of the node
         */
        def key (self) -> c8 {
            self._key
        }

        /**
         * @returns: true, if this token is a skip token
         */
        def isSkip (self)-> bool {
            self._isSkip
        }

        /**
         * @returns: a closing token, is this token is a stater of a comment, ""s8 otherwise
         */
        def isComment (self)-> [c8] {
            self._isComment
        }                
        
    }

}
