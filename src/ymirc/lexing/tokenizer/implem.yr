/*
 * Declare the Tokenizer record that implement the tokenization of a string to
 * tokens
 *
 * <hr>
 * 
 * @Package: ymirc        
 * @Module: ymirc::lexing::tokenizer::implem
 * @File: ymirc/lexing/tokenizer/implem.yr
 * @Author(s):
 *   - Emile Cadorel <ecadorel@gmail.com>
 *
 * @Created (YYYY-MM-DD): 2026-02-09
 * @Copyright (C) 2021â€“2026 GNU-Ymir
 * @License
 *   ```text
 *   This file is part of the Ymir language project.
 *
 *   Ymir is free software: you can redistribute it and/or modify it
 *   under the terms of the GNU General Public License as published by
 *   the Free Software Foundation, either version 3 of the License, or
 *   (at your option) any later version.
 *
 *   Ymir is distributed in the hope that it will be useful,
 *   but WITHOUT ANY WARRANTY; without even the implied warranty of
 *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *   GNU General Public License for more details.
 *
 *   You should have received a copy of the GNU General Public License
 *   along with Ymir.  If not, see <https://www.gnu.org/licenses/>.
 *   ```
 */

in implem;


pub record Tokenizer {

    // The tokens heads of the tokenizer
    let dmut _heads : [&node::Node] = [];

    /**
     * Create a new tokenizer, with a set of tokens
     * 
     * @params:
     *   - tokens: the list of token that will split the string
     */
    pub self (tokens : [[c8]]) {
        for i in tokens {
            self:.insert (i, isSkip-> false, isComment-> false, isString-> false);
        }
    }

    /*!
     * ====================================================================================================
     * ====================================================================================================
     * ===================================          INSERTION          ====================================
     * ====================================================================================================
     * ====================================================================================================
     */

    /**
     * Insert a new token in the tokenizer
     * 
     * @params:
     *    - token: the token to insert
     *    - isSkip: true iif the token is a skippable token
     *    - isComment: true iif the token is a comment opening token
     *    - isString: true iif the token is a string opening token     
     * */
    pub fn insert (mut self, token : [c8], isSkip : bool = false, isComment : bool = false, isString : bool = false) {
        if token.len != 0 {
            for i in 0 .. self._heads.len {
                if self._heads [i].key == token [0] {
                    self._heads [i]:.insert (token [1 .. $], isSkip-> isSkip, isComment-> isComment, isString-> isString);
                    return;
                }
            }

            let dmut n = copy node::Node (token [0]);
            n:.insert (token [1 .. $], isSkip-> isSkip, isComment-> isComment, isString-> isString);
            self._heads ~= [alias n];

            // Need to sort head, we do dichotomic search to find tokens
            std::algorithm::sorting::sort (alias self._heads, |x, y| => { x.key < y.key });
        }
    }

    /*!
     * ====================================================================================================
     * ====================================================================================================
     * ====================================          CUTTING          =====================================
     * ====================================================================================================
     * ====================================================================================================
     */


    /**
     * Find the size of the next token in a string, or the number of char before
     * reaching the next token
     * 
     * @params:
     *    - str: a string to read
     * @returns:
     *   - .0: the length of the next token inside the str
     *   - .1: true iif it's a token
     *   - .2: true iif the token is a skip token
     *   - .3: true iif this is a comment opening token
     *   - .4: true iif this is a string opening token
     * */
    pub fn next (self, str : [c8])-> (usize, bool, bool, bool, bool) {
        for s in 0 .. str.len {
            if let Ok (i) = self.find (str [s]) {
                let (len, isSkip, isComment, isString) = self._heads [i].len (str [s + 1 .. $]); // get the length of the token

                if len != 0 { // if the len is 0, then it is not really a token, it just starts like one
                    // does not start with a token, and a token was found at index s
                    if s != 0 { return (s, false, false, false, false); }

                    // else a token is directly found at index = 0
                    return (len, true, isSkip, isComment, isString);
                }
            }
        }

        // no token in str
        (str.len, false, false, false, false)
    }

    /*!
     * ====================================================================================================
     * ====================================================================================================
     * ====================================          PRIVATES          ====================================
     * ====================================================================================================
     * ====================================================================================================
     */

    /**
     * Search a token head in the list of token heads
     * 
     * @params:
     *    - c: the start of the token
     * @returns: the token index or none
     * */
    prv fn find (self, c : c8)-> (usize)? {
        if self._heads.len == 0 {
            return none;
        }

        let mut begin = 0is, mut end = cast!isize (self._heads.len) - 1;
        while begin <= end {
            let center = (begin + end) / 2;
            if self._heads [center].key == c {
                return (cast!usize (center))?;
            } else {
                if c > self._heads [center].key {
                    begin = center + 1;
                } else {
                    end = center - 1;
                }
            }
        }

        none
    }

}
